:_mod-docs-content-type: ASSEMBLY
include::_attributes/common-attributes.adoc[]
[id="virt-4-12-release-notes"]
= {VirtProductName} release notes
:context: virt-4-12-release-notes

toc::[]

== About Red Hat {VirtProductName}

Red Hat {VirtProductName} enables you to bring traditional virtual machines (VMs) into {product-title} where they run alongside containers, and are managed as native Kubernetes objects.

{VirtProductName} is represented by the image:virt-icon.png[{VirtProductName},40,40] icon.

You can use {VirtProductName} with either the xref:../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes] or the xref:../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[OpenShiftSDN] default Container Network Interface (CNI) network provider.

Learn more about xref:../virt/about-virt.adoc#about-virt[what you can do with {VirtProductName}].

Learn more about xref:../virt/virt-architecture.adoc#virt-how-virt-works_virt-architecture[{VirtProductName} architecture and deployments].

xref:../virt/install/preparing-cluster-for-virt.adoc#preparing-cluster-for-virt[Prepare your cluster] for {VirtProductName}.

include::modules/virt-supported-cluster-version.adoc[leveloffset=+2]


[id="virt-guest-os"]
=== Supported guest operating systems
//CNV-16390 Supported guest operating systems
To view the supported guest operating systems for {VirtProductName}, refer to link:https://access.redhat.com/articles/973163#ocpvirt[Certified Guest Operating Systems in Red Hat OpenStack Platform, Red Hat Virtualization and OpenShift Virtualization].


[id="virt-4-12-new"]
== New and changed features

//CNV-18488 SVVP for 4.12: Ensure platform passes Windows Server Virtualization Validation Program - with RHCOS workers
//NOTE: This is a recurring release note. Modify the existing note text below if recommended by QE.
* OpenShift Virtualization is certified in Microsoft's Windows Server Virtualization Validation Program (SVVP) to run Windows Server workloads.
+
The SVVP Certification applies to:
+
** Red Hat Enterprise Linux CoreOS workers. In the Microsoft SVVP Catalog, they are named __Red{nbsp}Hat {product-title} 4.12__.
** Intel and AMD CPUs.


//CNV-21611-2
* {VirtProductName} no longer uses the image:Operator_Icon-OpenShift_Virtualization-5.png[{VirtProductName},40,40] logo. {VirtProductName} is now represented by the image:virt-icon.png[{VirtProductName},40,40] logo for versions 4.9 and later.

//CNV-15830 Memory dump.
* You can create a VM memory dump for forensic analysis by using the xref:../virt/virt-using-the-cli-tools.adoc#vm-memory-dump-commands_virt-using-the-cli-tools[`virtctl memory-dump` command].

//CNV-20148 VM export
* You can xref:../virt/virtual_machines/virt-exporting-vms.adoc#virt-exporting-vms[export and download a volume] from a virtual machine (VM), a VM snapshot, or a persistent volume claim (PVC) to recreate it on a different cluster or in a different namespace on the same cluster by using the `virtctl vmexport` command or by creating a `VirtualMachineExport` custom resource. You can also export the memory-dump for forensic analysis.

// no Jira - Web overview page
* You can learn about the functions and organization of the {VirtProductName} web console by referring to the xref:../virt/virt-web-console-overview.adoc#virt-web-console-overview[web console overview documentation].

//CNV-16809
* You can use the `virtctl ssh` command to forward SSH traffic to a virtual machine by xref:../virt/virtual_machines/virt-accessing-vm-consoles.adoc#virt-accessing-vmi-ssh_virt-accessing-vm-consoles[using your local SSH client] or by xref:../virt/virtual_machines/virt-accessing-vm-consoles.adoc#virt-copying-the-ssh-command_virt-accessing-vm-consoles[copying the SSH command] from the {product-title} web console.

//CNV-19780 CNV Maturity and dataVolumeTemplates
* Standalone data volumes, and data volumes created when using a `dataVolumeTemplate` to prepare a disk for a VM, are no longer stored in the system. The data volumes are now automatically garbage collected and deleted after the PVC is created.

//CNV-20483 New virt migration metrics
* {VirtProductName} now provides xref:../virt/logging_events_monitoring/virt-prometheus-queries.adoc#virt-live-migration-metrics_virt-prometheus-queries[live migration metrics] that you can access by using the {product-title} monitoring dashboard.

//CNV-20181 Comply with cluster-wide crypto policy
* The {VirtProductName} Operator now reads the cluster-wide xref:../security/tls-security-profiles.adoc#tls-profiles-kubernetes-configuring_tls-security-profiles[TLS security profile] from the `APIServer` custom resource and propagates it to the {VirtProductName} components, including virtualization, storage, networking, and infrastructure.

//CNV-16414
* {VirtProductName} has xref:../virt/logging_events_monitoring/virt-runbooks.adoc#virt-runbooks[runbooks] to help you troubleshoot issues that trigger alerts. The alerts are displayed on the *Virtualization* -> *Overview* page of the web console. Each runbook defines an alert and provides steps to diagnose and resolve the issue. This feature was previously introduced as a Technology Preview and is now generally available.


[id="virt-4-12-quick-starts"]
=== Quick starts

* Quick start tours are available for several {VirtProductName} features. To view the tours, click the *Help* icon *?* in the menu bar on the header of the {VirtProductName} console and then select *Quick Starts*. You can filter the available tours by entering the `virtualization` keyword in the *Filter* field.


//[id="virt-4-12-installation-new"]
//=== Installation


[id="virt-4-12-networking-new"]
=== Networking

//CNV-20428
* You can now xref:../virt/logging_events_monitoring/virt-running-cluster-checkups.adoc#virt-about-cluster-checkup-framework_virt-running-cluster-checkups[specify the namespace] where the {product-title} cluster checkup is to be run.

//CNV-18349
* You can now xref:../virt/virtual_machines/vm_networking/virt-creating-service-vm.adoc#virt-about-services_virt-creating-service-vm[configure a load balancing service] by using the xref:../networking/metallb/metallb-operator-install.adoc#metallb-operator-install[MetalLB Operator] in layer 2 mode.


//[id="virt-4-12-storage-new"]
//=== Storage


[id="virt-4-12-web-new"]
=== Web console
//CNV-20482 Overview page usability enhancements
* The *Virtualization -> Overview* page has the following usability enhancements:

** A *Download virtctl* link is available.
** Resource information is customized for administrative and non-administrative users. For example, non-administrative users see only their VMs.
** The *Overview* tab displays the number of VMs, and vCPU, memory, and storage usage with charts that show the last 7 days' trend.
** The *Alerts* card on the *Overview* tab displays the alerts grouped by severity.
** The *Top Consumers* tab displays the top consumers of CPU, memory, and storage usage over a configurable time period.
** The *Migrations* tab displays the progress of VM migrations.
** The *Settings* tab displays cluster-wide settings, including live migration limits, live migration network, and templates project.

//CNV-18453 Live migration with shared storage for work intensive workloads
* You can create and manage live migration policies in a single location on the *Virtualization -> MigrationPolicies* page.

//CNV-18769 Add metrics tab to single VM view
* The *Metrics* tab on the *VirtualMachine details* page displays memory, CPU, storage, network, and migration metrics of a VM, over a configurable period of time.

//CNV-20936 UI - live YAML view
* When you customize a template to create a VM, you can set the *YAML* switch to *ON* on each VM configuration tab to view the live changes in the YAML configuration file alongside the form.

//CNV-18432 UI create live migration page
* The *Migrations* tab on the *Virtualization -> Overview* page displays the progress of virtual machine instance migrations over a configurable time period.

//CNV-17809 Live migration with shared storage for work intensive workloads
* You can now define a dedicated network for live migration to minimize disruption to tenant workloads. To select a network, navigate to *Virtualization* -> *Overview* -> *Settings* -> *Live migration*.


//NOTE: Comment out deprecated and removed features (and their IDs) if not used in a release
//[id="virt-4-12-deprecated-removed"]
//== Deprecated and removed features


[id="virt-4-12-deprecated"]
=== Deprecated features
// NOTE: when uncommenting deprecated features list, change the header level below to ===

Deprecated features are included in the current release and supported. However, they will be removed in a future release and are not recommended for new deployments.




[id="virt-4-12-removed"]
=== Removed features

Removed features are not supported in the current release.

// CNV-23499_412RN
* Support for the legacy HPP custom resource, and the associated storage class, has been removed for all new deployments. In {VirtProductName} {VirtVersion}, the HPP Operator uses the Kubernetes Container Storage Interface (CSI) driver to configure local storage. A legacy HPP custom resource is supported only if it had been installed on a previous version of {VirtProductName}.

//CNV-16317 NMState is no longer part of CNV
* {VirtProductName} 4.11 removed support for link:https://nmstate.io/[nmstate], including the following objects:
+
--
** `NodeNetworkState`
** `NodeNetworkConfigurationPolicy`
** `NodeNetworkConfigurationEnactment`
--
+
To preserve and support your existing nmstate configuration, install the xref:../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc#k8s-nmstate-about-the-k8s-nmstate-operator[Kubernetes NMState Operator] before updating to {VirtProductName} 4.11. For {VirtVersion} for xref:../virt/upgrading-virt.adoc#virt-about-eus-updates_upgrading-virt[Extended Update Support (EUS)] versions, install the Kubernetes NMState Operator after updating to {VirtVersion}. You can install the Operator from the *OperatorHub* in the {product-title} web console, or by using the OpenShift CLI (`oc`).

* The Node Maintenance Operator (NMO) is no longer shipped with {VirtProductName}. You can install the NMO from the *OperatorHub* in the {product-title} web console, or by using the OpenShift CLI (`oc`).
+
You must perform one of the following tasks before updating to {VirtProductName} 4.11 from {VirtProductName} 4.10.2 and later 4.10 releases. For xref:../virt/upgrading-virt.adoc#virt-about-eus-updates_upgrading-virt[Extended Update Support (EUS)] versions, you must perform the following tasks before updating to {VirtProductName} {VirtVersion} from 4.10.2 and later 4.10 releases:

** Move all nodes out of maintenance mode.
** Install the standalone NMO and replace the `nodemaintenances.nodemaintenance.kubevirt.io` custom resource (CR) with a `nodemaintenances.nodemaintenance.medik8s.io` CR.


//[id="virt-4-12-changes"]
//== Notable technical changes


[id="virt-4-12-technology-preview"]
== Technology Preview features

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use. Note the following scope of support on the Red Hat Customer Portal for these features:

link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]

//CNV-21824
* You can now run xref:../virt/logging_events_monitoring/virt-running-cluster-checkups.adoc#virt-measuring-latency-vm-secondary-network_virt-running-cluster-checkups[{product-title} cluster checkups] to measure network latency between VMs.

//CNV-20526
* The Tekton Tasks Operator (TTO) now xref:../virt/virtual_machines/virt-managing-vms-openshift-pipelines.adoc#virt-managing-vms-openshift-pipelines[integrates {VirtProductName} with {pipelines-title}]. TTO includes cluster tasks and example pipelines that allow you to:
+
** Create and manage virtual machines (VMs), persistent volume claims (PVCs), and data volumes.
** Run commands in VMs.
** Manipulate disk images with `libguestfs` tools.
** Install Windows 10 into a new data volume from a Windows installation image (ISO file).
** Customize a basic Windows 10 installation and then create a new image and template.

//CNV-20149
* You can now use the xref:../virt/logging_events_monitoring/virt-monitoring-vm-health.adoc#virt-define-guest-agent-ping-probe_virt-monitoring-vm-health[guest agent ping probe] to determine if the QEMU guest agent is running on a virtual machine.

//CNV-20963
* You can now use Microsoft Windows 11 as a guest operating system. However, {VirtProductName} {VirtVersion} does not support USB disks, which are required for a critical function of BitLocker recovery. To protect recovery keys, use other methods described in the link:https://learn.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-recovery-guide-plan[BitLocker recovery guide].

//CNV-19145 Support live migration policies
* You can create live migration policies with specific parameters, such as bandwidth usage, maximum number of parallel migrations, and timeout, and apply the policies to groups of virtual machines by using virtual machine and namespace labels.


[id="virt-4-12-bug-fixes"]
== Bug fixes

* You can now configure the `HyperConverged` CR to enable mediated devices before drivers are installed without losing the new device configuration after driver installation. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2046298[*BZ#2046298*])

* The OVN-Kubernetes cluster network provider no longer crashes from peak RAM and CPU usage if you create a large number of `NodePort` services. (link:https://issues.redhat.com/browse/OCPBUGS-1940[*OCPBUGS-1940*])

* Cloning more than 100 VMs at once no longer intermittently fails if you use Red Hat Ceph Storage or Red Hat OpenShift Data Foundation Storage. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1989527[*BZ#1989527*])


[id="virt-4-12-known-issues"]
== Known issues

* You cannot run {VirtProductName} on a single-stack IPv6 cluster. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2193267[*BZ#2193267*])

* In a heterogeneous cluster with different compute nodes, virtual machines that have HyperV Reenlightenment enabled cannot be scheduled on nodes that do not support timestamp-counter scaling (TSC) or have the appropriate TSC frequency. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2151169[*BZ#2151169*])

* When you use two pods with different SELinux contexts, VMs with the `ocs-storagecluster-cephfs` storage class fail to migrate and the VM status changes to `Paused`. This is because both pods try to access the shared `ReadWriteMany` CephFS volume at the same time. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2092271[*BZ#2092271*])
** As a workaround, use the `ocs-storagecluster-ceph-rbd` storage class to live migrate VMs on a cluster that uses Red Hat Ceph Storage.

// Fix targeted for 4.12.1
* The `TopoLVM` provisioner name string has changed in {VirtProductName} 4.12. As a result, the automatic import of operating system images might fail with the following error message (link:https://bugzilla.redhat.com/show_bug.cgi?id=2158521[*BZ#2158521*]):
+
[source,terminal]
----
DataVolume.storage spec is missing accessMode and volumeMode, cannot get access mode from StorageProfile.
----
** As a workaround:
. Update the `claimPropertySets` array of the storage profile:
+
[source,terminal]
----
$ oc patch storageprofile <storage_profile> --type=merge -p '{"spec": {"claimPropertySets": [{"accessModes": ["ReadWriteOnce"], "volumeMode": "Block"}, \
    {"accessModes": ["ReadWriteOnce"], "volumeMode": "Filesystem"}]}}'
----
. Delete the affected data volumes in the `openshift-virtualization-os-images` namespace. They are recreated with the access mode and volume mode from the updated storage profile.

// Fix targeted for 4.13
* When restoring a VM snapshot for storage whose binding mode is `WaitForFirstConsumer`, the restored PVCs remain in the `Pending` state and the restore operation does not progress.
** As a workaround, start the restored VM, stop it, and then start it again. The VM will be scheduled, the PVCs will be in the `Bound` state, and the restore operation will complete. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2149654[*BZ#2149654*])

//New known issues for 4.12 go above this comment
* VMs created from common templates on a Single Node OpenShift (SNO) cluster display a `VMCannotBeEvicted` alert because the template's default eviction strategy is `LiveMigrate`. You can ignore this alert or remove the alert by updating the VM's eviction strategy. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2092412[*BZ#2092412*])

* Uninstalling {VirtProductName} does not remove the `feature.node.kubevirt.io` node labels created by {VirtProductName}. You must remove the labels manually. (link:https://issues.redhat.com/browse/CNV-22036[*CNV-22036*])

* Some persistent volume claim (PVC) annotations created by the Containerized Data Importer (CDI) can cause the virtual machine snapshot restore operation to hang indefinitely. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2070366[*BZ#2070366*])
** As a workaround, you can remove the annotations manually:
. Obtain the xref:../virt/virtual_machines/virtual_disks/virt-managing-vm-snapshots.adoc#virtual-machine-snapshot-controller-and-custom-resource-definitions-crds[VirtualMachineSnapshotContent] custom resource (CR) name from the `status.virtualMachineSnapshotContentName` value in the `VirtualMachineSnapshot` CR.
. Edit the `VirtualMachineSnapshotContent` CR and remove all lines that contain `k8s.io/cloneRequest`.
. If you did not specify a value for `spec.dataVolumeTemplates` in the `VirtualMachine` object, delete any `DataVolume` and `PersistentVolumeClaim` objects in this namespace where both of the following conditions are true:
+
.. The object's name begins with `restore-`.
.. The object is not referenced by virtual machines.
+
This step is optional if you specified a value for `spec.dataVolumeTemplates`.
. Repeat the xref:../virt/virtual_machines/virtual_disks/virt-managing-vm-snapshots.adoc#virt-restoring-vm-from-snapshot-cli_virt-managing-vm-snapshots[restore operation] with the updated `VirtualMachineSnapshot` CR.

* Windows 11 virtual machines do not boot on clusters running in link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/security_hardening/index#con_federal-information-processing-standard-fips_assembly_installing-the-system-in-fips-mode[FIPS mode]. Windows 11 requires a TPM (trusted platform module) device by default. However, the `swtpm` (software TPM emulator) package is incompatible with FIPS. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2089301[*BZ#2089301*])

//BZ-1885605
* If your {product-title} cluster uses OVN-Kubernetes as the default Container Network Interface (CNI) provider, you cannot attach a Linux bridge or bonding device to a host's default interface because of a change in the host network topology of OVN-Kubernetes. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1885605[*BZ#1885605*])
** As a workaround, you can use a secondary network interface connected to your host, or switch to the OpenShift SDN default CNI provider.

* In some instances, multiple virtual machines can mount the same PVC in read-write mode, which might result in data corruption. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1992753[*BZ#1992753*])
** As a workaround, avoid using a single PVC in read-write mode with multiple VMs.

* The Pod Disruption Budget (PDB) prevents pod disruptions for migratable virtual machine images. If the PDB detects pod disruption, then `openshift-monitoring` sends a `PodDisruptionBudgetAtLimit` alert every 60 minutes for virtual machine images that use the `LiveMigrate` eviction strategy. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2026733[*BZ#2026733*])
** As a workaround, xref:../observability/monitoring/managing-alerts.adoc#silencing-alerts_managing-alerts[silence alerts].

* {VirtProductName} links a service account token in use by a pod to that specific pod. {VirtProductName} implements a service account volume by creating a disk image that contains a token. If you migrate a VM, then the service account volume becomes invalid. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2037611[*BZ#2037611*])
** As a workaround, use user accounts rather than service accounts because user account tokens are not bound to a specific pod.

* If you clone more than 100 VMs using the `csi-clone` cloning strategy, then the Ceph CSI might not purge the clones. Manually deleting the clones can also fail. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2055595[*BZ#2055595*])
** As a workaround, you can restart the `ceph-mgr` to purge the VM clones.

* VMs that use Logical volume management (LVM) with block storage devices require additional configuration to avoid conflicts with {op-system-first} hosts.
** As a workaround, you can create a VM, provision an LVM, and restart the VM. This creates an empty `system.lvmdevices` file. (link:https://issues.redhat.com/browse/OCPBUGS-5223[*OCPBUGS-5223*])
